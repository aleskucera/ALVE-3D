# ALVE-3D

[//]: # (![Image generated by DreamStudio]&#40;images/2647784599_Colorful_3D_scene_of_point_cloud_computer_vision.png&#41;)

## Introduction

This code is official implementation of the project **ALVE-3D (Active Learning with Viewpoint Entropy
for 3D Semantic Segmentation)**. We propose a novel active learning framework for 3D semantic segmentation based on the
viewpoint entropy.
The framework is will be evaluated on SemanticKITTI and SemanticUSL datasets.

## Requirements

- Python 3.9

for all other requirements, please see `environment.yaml`. You can recreate the environment with:

    conda env create -f environment.yaml

## Repository Structure

This is the main structure of the repository:

    .
    ├── conf
    ├── data
    │   ├── SemanticKITTI -> /path/to/SemanticKITTI
    │   └── SemanticUSL -> /path/to/SemanticUSL
    ├── models
    │   └── pretrained
    ├── scripts
    ├── src
    └── main.py

- `conf`: This folder contains the configuration files for the experiments. The configuration files are in YAML format.
  The project depends on [Hydra](https://hydra.cc/) for configuration management.
- `data`: This folder contains the symbolic links to the datasets.
  It is recommended to create symbolic links to the datasets in this folder, but you can also change the paths
  in the configuration files.
- `models`: This folder contains models that are used in the experiments.
  For evaluation of the pretrained models, please use the pretrained directory.
- `scripts`: This folder contains the scripts for training, evaluation and visualization of the models.
- `src`: This folder contains the source code of the project.
- `main.py`: This is the main script of the project. It is used for training, evaluation and visualization of the
  models.

## Usage

For creation dataset and global map of the SemanticKITTI dataset, please run the following command:

    python main.py

For visualization of the dataset, please run the following command:

    python src/dataset/visualize.py --dataset /path/to/dataset

## Dataset

The object SemanticDataset is Pytorch Dataset wrapper for SemanticKITTI and SemanticUSL datasets.
It is used for loading the data and creating the global map of the dataset.

Dataset uses two new [dataclasses](https://docs.python.org/3/library/dataclasses.html) for storing the data:

- `Sample`: This dataclass is used for storing the data of a single sample. It contains everything that is needed for
  training and evaluation of the model. For better performance, only the essential data are stored permanently in the
  dataset and the rest of are loaded on demand (e.g. point clouds, labels, etc.).
- `Sequence`: This dataclass is used for storing information about a structure of a single sequence. The structure of a
  sequence is defined by the `sequence_structure` parameter in the configuration file. The structure is used for
  creating
  the global map of the dataset.

### TODO:

- [x] Create dataset wrapper for SemanticKITTI and SemanticUSL datasets
- [x] Create global map of the dataset
- [ ] Visualize the global map of the dataset
- [ ] Add singularity directory for creating singularity image from environment.yaml
- [ ] Check scripts for training, evaluation and visualization of the models




