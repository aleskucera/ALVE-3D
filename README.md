# ALVE-3D

[//]: # (![Image generated by DreamStudio]&#40;images/2647784599_Colorful_3D_scene_of_point_cloud_computer_vision.png&#41;)

## Introduction

This code is official implementation of the project **ALVE-3D (Active Learning with Viewpoint Entropy
for 3D Semantic Segmentation)**. We propose a novel active learning framework for 3D semantic segmentation based on the
viewpoint entropy.
The framework is will be evaluated on SemanticKITTI and SemanticUSL datasets.

## Requirements

- Python 3.9

for all other requirements, please see `environment.yaml`. You can recreate the environment with:

    conda env create -f environment.yaml

## Repository Structure

This is the main structure of the repository:

    .
    ├── conf
    ├── data
    │   ├── SemanticKITTI -> /path/to/SemanticKITTI
    │   └── SemanticUSL -> /path/to/SemanticUSL
    ├── models
    │   └── pretrained
    ├── scripts
    ├── src
    └── main.py

- `conf`: This folder contains the configuration files for the experiments. The configuration files are in YAML format.
  The project depends on [Hydra](https://hydra.cc/) for configuration management.
- `data`: This folder contains the symbolic links to the datasets.
  It is recommended to create symbolic links to the datasets in this folder, but you can also change the paths
  in the configuration files.
- `models`: This folder contains models that are used in the experiments.
  For evaluation of the pretrained models, please use the pretrained directory.
- `scripts`: This folder contains the scripts for training, evaluation and visualization of the models.
- `src`: This folder contains the source code of the project.
- `main.py`: This is the main script of the project. It is used for training, evaluation and visualization of the
  models.

## Demo

There are 3 demos in the repository at the moment: `global_cloud`, `sample` and `formats`. You can run the demos with:

    python main.py demo=<demo_name>

or you can change the `demo` parameter in the configuration files.

## Dataset

The object SemanticDataset is Pytorch Dataset wrapper for SemanticKITTI and SemanticUSL datasets.
It is used for loading the data and creating the global map of the dataset.

Dataset uses two new [dataclasses](https://docs.python.org/3/library/dataclasses.html) for storing the data:

- `Sample`: This dataclass is used for storing the data of a single sample. It contains everything that is needed for
  training and evaluation of the model. For better performance, only the essential data are stored permanently in the
  dataset and the rest of are loaded on demand (e.g. point clouds, labels, etc.). More information about the dataclass
  can be found in the Sample section.
- `Sequence`: This dataclass is used for storing information about a structure of a single sequence. The structure of a
  sequence is defined by the `sequence_structure` parameter in the configuration file. The structure is used for
  creating
  the global map of the dataset.

## Sample class

The `Sample` class is used for storing the data of a single sample. It contains everything that is needed for training
and visualization of the dataset.
For better performance, only the essential data are stored permanently in the dataset and the rest of are loaded on
demand (e.g. point clouds, labels, etc.).
There are 3 main types of data that can be loaded in the `Sample` class:

- `learning_data`: This data are used for training and evaluation of the model. The data are loaded from the dataset
  and stored permanently in the `Sample` class by function `load_learning_data`.
- `semantic_cloud_data`: This data are used for visualization of the semantic point cloud. The data are loaded from the
  dataset
  and stored permanently in the `Sample` class by function `load_semantic_cloud`.
- `depth_image_data`: This data are used for visualization of the depth image. The data are loaded from the dataset
  and stored permanently in the `Sample` class by function `load_depth_image`.

## Sequence class

The `Sequence` class is used for storing information about a structure of a single sequence. The structure
of a sequence is defined by the `sequence_structure` parameter in the configuration file. It is used
loading the data and creating `Sample` objects by calling the `get_samples` function.

### TODO:

- [x] Create dataset wrapper for SemanticKITTI and SemanticUSL datasets
- [x] Create global map of the dataset
- [ ] Visualize the global map of the dataset
- [ ] Add singularity directory for creating singularity image from environment.yaml
- [ ] Check scripts for training, evaluation and visualization of the models




